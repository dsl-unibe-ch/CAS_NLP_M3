{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c945f271",
   "metadata": {},
   "source": [
    "<b>Simple script to train Hidden Markov Model for Part of Speech tagging using NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42651d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the models\n",
    "import nltk\n",
    "from nltk import HiddenMarkovModelTagger as hmm # do not use nltk.tag.hmm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "import warnings\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f2a6f",
   "metadata": {},
   "source": [
    "<b>Download the data. Run only once</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the treebank dataset is downloaded\n",
    "#nltk.download('treebank')\n",
    "#nltk,download('punkt')\n",
    "#nltk,download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5837486",
   "metadata": {},
   "source": [
    "<b>Prepare the data. We'll use the Penn Treebank which is an English Corpus that includes pos tagging. For information on the tagset: https://www.sketchengine.eu/penn-treebank-tagset/\n",
    "We split the data into training and testing. Try to change the data size and experiment with the accuracy change.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of tagged examples in the dataset is: {len(brown.tagged_sents(tagset=\"universal\"))}')\n",
    "train_data = brown.tagged_sents(tagset='universal')[:50000]\n",
    "test_data = brown.tagged_sents(tagset='universal')[50000:]\n",
    "\n",
    "print(f'len of training data is {len(train_data)}')\n",
    "print(f'len of testing data is {len(test_data)}')\n",
    "print(train_data[0])\n",
    "\n",
    "# Extracting unique tags from train_data\n",
    "unique_tags = set(tag for sent in train_data for _, tag in sent)\n",
    "\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9059b0",
   "metadata": {},
   "source": [
    "<b>Define the trainer and train the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727989d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = hmm.train(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's accuracy on the test data\n",
    "accuracy = tagger.accuracy(test_data)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c09573",
   "metadata": {},
   "source": [
    "<b>Generate true tags list and model prediction to get more detailed stats on where the model performed better and where it didn't perform so well</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe468aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "true_tags = [tag for sent in test_data for _, tag in sent]\n",
    "predicted_tags = [tag for sent in tagger.tag_sents([[word for word, _ in sent] for sent in test_data]) for _, tag in sent]\n",
    "print(true_tags[20:40])\n",
    "print(predicted_tags[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83de852-2549-4b35-b969-f4c0d79a1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(true_tags, predicted_tags)\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(true_tags, predicted_tags))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_tags, predicted_tags, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_accuracy_dict = {}\n",
    "\n",
    "for label in labels:\n",
    "    correct_predictions = sum(1 for t, p in zip(true_tags, predicted_tags) if t == label and p == label)\n",
    "    total_predictions = sum(1 for t in true_tags if t == label)\n",
    "    wrong_predictions = total_predictions - correct_predictions\n",
    "    label_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Store each label's accuracy in the dictionary\n",
    "    label_accuracy_dict[label] = label_accuracy\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Wrong Predictions: {wrong_predictions}\")\n",
    "    print(f\"Accuracy: {label_accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5f670-328a-4d9e-ab72-2ac2f1a263aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of accuracy for each label\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(label_accuracy_dict.keys(), label_accuracy_dict.values(), color='skyblue')\n",
    "plt.xlabel('Tags')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Each Tag')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e62b6",
   "metadata": {},
   "source": [
    "<b>If I'm happy with the model, I can save it for later usage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open('hmm_tagger.pkl', 'wb') as f:\n",
    "    dill.dump(tagger, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cb897",
   "metadata": {},
   "source": [
    "<b>You can load the model at anytime to use it for tagging sentences</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed608f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the file\n",
    "with open('hmm_tagger.pkl', 'rb') as f:\n",
    "    loaded_tagger = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f982b79",
   "metadata": {},
   "source": [
    "<b>Try the model on new text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b096dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I took the train from Zurich to Italy last night'\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Tag the tokenized sentence\n",
    "tagged_sentence = loaded_tagger.tag(tokens)\n",
    "\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93233e-9f29-4305-843b-75f874a8b04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
