{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25d40f2",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing with NLTK\n",
    "\n",
    "This Python script provides an introductory guide to using the Natural Language Toolkit (NLTK),\n",
    "a popular library for natural language processing (NLP) in Python. It covers basic NLP tasks such as\n",
    "tokenization, part-of-speech (POS) tagging, lemmatization, sentiment analysis, and other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec33d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data. Do it only once\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7652aa",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# Tokenization\n",
    "# --------------------\n",
    "\n",
    "Tokenization is the process of breaking the text into words, phrases, symbols, or other meaningful elements called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello! This is an example text. Let's see how NLTK handles this.\"\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print('Word Tokens:', word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print('Sentence Tokens:', sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8eb949",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# POS Tagging\n",
    "# --------------------\n",
    "Part-of-Speech Tagging assigns parts of speech to each word (such as nouns, verbs, adjectives, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de871049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tag(word_tokens)\n",
    "print('POS Tags:', pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d22094",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# Lemmatization\n",
    "# --------------------\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640434dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "lemmatized_word = lemmatizer.lemmatize('running', pos='v')  # Verb\n",
    "print('Lemmatized Word:', lemmatized_word)\n",
    "lemmatized_word = lemmatizer.lemmatize('went', pos='v')\n",
    "print('Lemmatized Word:', lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd869b1b",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# Sentiment Analysis\n",
    "# --------------------\n",
    "\n",
    "Sentiment Analysis is the process of determining the emotional tone behind a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79725f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sentence_1 = \"I love natural language processing!\"\n",
    "sentiment_score = sia.polarity_scores(sentence_1)\n",
    "print('Sentiment Score for sentence_1:', sentiment_score)\n",
    "\n",
    "sentence_2 = \"I hate math! It is the worst subject\"\n",
    "sentiment_score = sia.polarity_scores(sentence_2)\n",
    "print('Sentiment Score for sentence_2:', sentiment_score)\n",
    "\n",
    "sentence_3 = \"The new Samsung is very expenisve but the camera is really great!\"\n",
    "sentiment_score = sia.polarity_scores(sentence_3)\n",
    "print('Sentiment Score for sentence_3:', sentiment_score)\n",
    "\n",
    "sentence_4 = \"I bought this phone 3 weeks ago. The camera stopped working last week.\"\n",
    "sentiment_score = sia.polarity_scores(sentence_4)\n",
    "print('Sentiment Score for sentence_4:', sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1411e13",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# NER\n",
    "# --------------------\n",
    "\n",
    "Named Entity Recognition (NER) is a process in natural language processing that identifies and classifies named entities within text into predefined categories such as names of persons, organizations, locations, dates, quantities, and monetary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for analysis\n",
    "text = \"\"\"New York City is one of the largest cities in the US and worldwid. \n",
    "          The Knicks is the largest basketball team in the city. \n",
    "          It won the championship in the 1970 and 1973.\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS Tagging\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner_result = ne_chunk(tags)\n",
    "print(\"Named Entity Recognition:\")\n",
    "print(ner_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f977b",
   "metadata": {},
   "source": [
    "# --------------------\n",
    "# Accessing Corpora \n",
    "# --------------------\n",
    "\n",
    "NLTK provides many high-quality corpora that can be used for many purposes and accessed easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use WordNet as an example to access synsets for a word\n",
    "synsets = wordnet.synsets(\"computer\")\n",
    "print(\"WordNet Synsets for 'computer':\", synsets)\n",
    "\n",
    "# Frequency Distribution\n",
    "fdist = FreqDist(tokens)\n",
    "print(\"Frequency Distribution for the top 5 words:\")\n",
    "print(fdist.most_common(5))\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
    "print(\"Filtered Sentence without Stop Words:\", filtered_sentence)\n",
    "\n",
    "# n-grams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a239b",
   "metadata": {},
   "source": [
    "<b> What else can you do with NLTK?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685511d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
