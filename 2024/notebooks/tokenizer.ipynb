{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062630f5-f672-4335-9080-34067054b0f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "<b> Rule-based Tokenizer:  Notebook to demonstrate simple rule-based tokenizer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4ad207-b489-4fc8-980d-64389a2a049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c93a08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # Rule 1: Replace every punctuation with whitespace then the punctuation\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, f' {punct}')\n",
    "    \n",
    "    # Rule 2: Replace every whitespace with new line character\n",
    "    text = text.replace(' ', '\\n')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967d779",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the text from the file\n",
    "    with open('../data/english.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(text)\n",
    "    \n",
    "    # Write the tokenized text to a new file\n",
    "    with open('english_tokenized.txt', 'w') as file:\n",
    "        file.write(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beb2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66abc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toy', 'giant', 'Lego', 'has', 'scrapped', 'plans', 'to', 'make', 'its', 'bricks', 'from', 'recycled', 'bottles', ',', 'in', 'a', 'blow', 'to', 'its', 'efforts', 'to', 'cut', 'carbon', 'emissions', '.', '', 'The', 'company', 'said', 'in', '2021', 'that', 'it', 'aimed', 'to', 'produce', 'bricks', 'not', 'containing', 'crude', 'oil', 'within', 'two', 'years', '.', '', 'But', 'on', 'Monday', ',', 'it', 'said', 'it', 'had', 'found', 'that', 'using', 'the', 'new', 'material', 'didn', \"'t\", 'reduce', 'carbon', 'emissions', '.', '', 'Lego', 'said', 'it', 'remains', '', '\"fully', 'committed', '\"', 'to', 'making', 'bricks', 'from', 'sustainable', 'materials', '.', '', 'Currently', ',', 'many', 'of', 'Lego', \"'s\", 'bricks', 'are', 'made', 'using', 'acrylonitrile', 'butadiene', 'styrene', '', '(ABS', ')', ',', 'a', 'virgin', 'plastic', 'made', 'from', 'crude', 'oil', '.', '', 'The', 'move', ',', 'which', 'was', 'first', 'reported', 'in', 'the', 'the', 'Financial', 'Times', ',', 'will', 'be', 'seen', 'as', 'a', 'setback', 'after', 'a', 'high', '-profile', 'push', 'by', 'Lego', 'to', 'improve', 'its', 'green', 'credentials', '.', '', 'Like', 'many', 'other', 'companies', ',', 'Lego', 'has', 'been', 'exploring', 'alternative', 'materials', 'to', 'plastic', 'as', 'sustainability', 'becomes', 'more', 'important', 'to', 'customers', '.', '', 'One', 'of', 'the', 'challenges', 'has', 'been', 'finding', 'a', 'material', 'that', 'is', 'durable', 'enough', 'to', 'last', 'for', 'generations', '.', '', 'In', '2021', ',', 'it', 'said', 'it', 'has', 'developed', 'prototype', 'bricks', 'made', 'from', 'polyethylene', 'terephthalate', '', '(PET', ')', 'bottles', ',', 'with', 'some', 'other', 'chemicals', 'added', '.', '', 'The', 'hope', 'was', 'that', 'material', 'could', 'have', 'offered', 'an', 'alternative', 'to', 'oil', '-based', 'bricks', '.', '', 'But', 'Lego', 'has', 'now', 'revealed', 'that', 'after', 'more', 'than', 'two', 'years', 'of', 'testing', ',', 'it', 'had', 'found', 'that', 'using', 'recycled', 'PET', 'didn', \"'t\", 'reduce', 'carbon', 'emissions', '.', '', 'It', 'said', 'the', 'reason', 'for', 'that', 'was', 'because', 'extra', 'steps', 'were', 'required', 'in', 'the', 'production', 'process', ',', 'which', 'meant', 'it', 'needed', 'to', 'use', 'more', 'energy', '.', '', 'As', 'a', 'result', ',', 'it', 'said', 'it', 'has', '', '\"decided', 'not', 'to', 'progress', '\"', 'with', 'making', 'bricks', 'from', 'the', 'material', '.', '', 'It', 'said', 'it', 'was', 'currently', 'testing', 'and', 'developing', 'bricks', 'made', 'from', '', '\"a', 'range', 'of', 'alternative', 'sustainable', 'materials', '\"', '.', '', 'Niels', 'Christiansen', ',', 'chief', 'executive', 'of', 'Lego', ',', 'told', 'the', 'FT', 'that', 'there', 'was', 'no', '', '\"magic', 'material', '\"', 'to', 'resolve', 'the', 'firm', \"'s\", 'sustainability', 'challenges', '.', '', '', '\"We', 'tested', 'hundreds', 'and', 'hundreds', 'of', 'materials', '.', 'It', \"'s\", 'just', 'not', 'been', 'possible', 'to', 'find', 'a', 'material', 'like', 'that', ',', '\"', 'he', 'said', '.', '', 'A', 'spokesperson', 'for', 'the', 'company', 'told', 'the', 'BBC', ':', '', '\"We', 'remain', 'fully', 'committed', 'to', 'making', 'Lego', 'bricks', 'from', 'sustainable', 'materials', 'by', '2032', '.', '', '', '\"We', 'are', 'investing', 'more', 'than', '', '$1', '.2bn', 'in', 'sustainability', 'initiatives', 'in', 'the', 'four', 'years', 'to', '2025', 'as', 'part', 'of', 'our', 'efforts', 'to', 'transition', 'to', 'more', 'sustainable', 'materials', 'and', 'reduce', 'our', 'carbon', 'emissions', 'by', '37', '%', 'by', '2032', '.', '\"', '', 'Dusty', 'samples', 'from', 'the', '', '\"most', 'dangerous', 'known', 'rock', 'in', 'the', 'Solar', 'System', '\"', 'have', 'been', 'brought', 'to', 'Earth', '.', '', 'The', 'American', 'space', 'agency', 'Nasa', 'landed', 'the', 'materials', 'in', 'a', 'capsule', 'that', 'came', 'down', 'in', 'the', 'West', 'Desert', 'of', 'Utah', 'state', '.', '', 'The', 'samples', 'had', 'been', 'scooped', 'up', 'from', 'the', 'surface', 'of', 'asteroid', 'Bennu', 'in', '2020', 'by', 'the', 'Osiris', '-Rex', 'spacecraft', '.', '', 'Nasa', 'wants', 'to', 'learn', 'more', 'about', 'the', 'mountainous', 'object', ',', 'not', 'least', 'because', 'it', 'has', 'an', 'outside', 'chance', 'of', 'hitting', 'our', 'planet', 'in', 'the', 'next', '300', 'years', '.', '', 'But', 'more', 'than', 'this', ',', 'the', 'samples', 'are', 'likely', 'to', 'provide', 'fresh', 'insights', 'into', 'the', 'formation', 'of', 'the', 'Solar', 'System', '4', '.6', 'billion', 'years', 'ago', 'and', 'possibly', 'even', 'how', 'life', 'got', 'started', 'on', 'our', 'world', '.', '', 'Osiris', '-Rex', ':', 'Asteroid', 'Bennu', '', \"'is\", 'a', 'journey', 'back', 'to', 'our', 'origins', \"'\", 'There', 'was', 'jubilation', 'when', 'the', 'Osiris', '-Rex', 'team', 'caught', 'sight', 'of', 'their', 'capsule', 'on', 'long', '-range', 'cameras', '.', 'source', ':', 'https', ':', '/', '/www', '.bbc', '.com', '/news', '/business', '-66910573']\n"
     ]
    }
   ],
   "source": [
    "with open('english_tokenized.txt', 'r') as file:\n",
    "    tokens = file.readlines()\n",
    "    tokens = [token.replace(\"\\n\", \"\") for token in tokens]\n",
    "    \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea9ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      "sentence\n",
      ".\n",
      "The\n",
      "tokenizer\n",
      "is\n",
      "ruled\n",
      "-based\n",
      ".\n",
      "how\n",
      "good\n",
      "is\n",
      "it\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "text_example = 'This is a test sentence. The tokenizer is ruled-based. how good is it?'\n",
    "\n",
    "print(tokenize_text(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a97f9-14a8-4816-8387-9a69b15ceebd",
   "metadata": {},
   "source": [
    "<b> Think of the casees where this approach would fail </b>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
