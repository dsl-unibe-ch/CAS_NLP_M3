{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825cdf17-2227-42f2-8073-654c01e4709c",
   "metadata": {},
   "source": [
    "#  Using Embeddings for Semantic Search\r\n",
    "\r\n",
    "In this notebook, we will explore how **embeddings** can be used to build a simple search engine for text data.  \r\n",
    "Instead of relying on keyword matching, embeddings allow us to capture the **semantic meaning** of text. This means that even if the words are not exactly the same, we can still find relevant results.\r\n",
    "\r\n",
    "### What you will learn\r\n",
    "- How to **load and preprocess text data** (in this case, a collection of books).  \r\n",
    "- How to use a **pretrained embedding model** (`SentenceTransformer`) to convert entire books into vectors.  \r\n",
    "- How to measure **similarity between a query and the books** using **cosine similarity**.  \r\n",
    "- How to build a simple **search function** that returns the most relevant books for a user’s query.  \r\n",
    "\r\n",
    "### Why embeddings?\r\n",
    "Traditional search engines often rely on keywords:  \r\n",
    "- Searching for *\"dog\"* will only find documents containing the exact word *\"dog\"*.  \r\n",
    "\r\n",
    "With embeddings, the model learns meaning:  \r\n",
    "- Searching for *\"puppy\"* may also retrieve documents about *dogs*, because the two are semantically related.  \r\n",
    "\r\n",
    "### Limitations of this demo\r\n",
    "- For simppart of the, we embed **entire books as single vectors**.  \r\n",
    "  In real-world systems, texts are usually split into **smaller chunks** (paragraphs or pages) for better retrieval.  \r\n",
    "- Computing embeddings for thousands of so the embedding is done in advacne and stored into pkl fileke st similar to your query.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246943e1-2031-4ac8-9f82-f564e6b4705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /storage/homefs/aa22x177/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!!pip install -upgrade sentence-transformers scikit-learn tqdm\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4496c8c-8904-4605-b06e-bb595fac2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: load the needed models. \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31557ece-b167-4844-ab08-69153ac06a5f",
   "metadata": {},
   "source": [
    "<b> First, we load the books, do some simple preprocessing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b13ed4-8667-49db-bc74-176cbcd5d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading books from 'data/'...\n",
      "####Loaded 5689 books.\n",
      "Processing and truncating books to 800 words...\n",
      "############### Snippets created.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Define key variables here for easy modification\n",
    "DATA_DIR = 'data/'\n",
    "EMBEDDING_FILE = 'book_snippet_embeddings.pkl'\n",
    "MAX_WORDS_PER_BOOK = 800  # We'll use the first 800 words of each book for the demo\n",
    "\n",
    "# Step 2: Define Functions for Data Loading and Preprocessing\n",
    "\n",
    "def load_books(data_dir):\n",
    "    \"\"\"Loads all .txt files from the specified directory into a dictionary.\"\"\"\n",
    "    books = {}\n",
    "    print(f\"Loading books from '{data_dir}'...\")\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                books[filename] = f.read()\n",
    "    print(f\"####Loaded {len(books)} books.\")\n",
    "    return books\n",
    "\n",
    "def preprocess_and_truncate(books):\n",
    "    \"\"\"\n",
    "    Cleans and truncates the text of each book to MAX_WORDS_PER_BOOK.\n",
    "    This creates the snippets we will embed.\n",
    "    \"\"\"\n",
    "    book_snippets = {}\n",
    "    print(f\"Processing and truncating books to {MAX_WORDS_PER_BOOK} words...\")\n",
    "    for filename, text in books.items():\n",
    "        # Remove non-alphabetic characters and convert to lowercase\n",
    "        processed_text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "        \n",
    "        # Split into words and take the first MAX_WORDS_PER_BOOK\n",
    "        words = processed_text.split()\n",
    "        snippet = \" \".join(words[:MAX_WORDS_PER_BOOK])\n",
    "        book_snippets[filename] = snippet\n",
    "        \n",
    "    print(\"############### Snippets created.\")\n",
    "    return book_snippets\n",
    "\n",
    "# Step 3: Load and Process the Data\n",
    "\n",
    "# Load the original full text of the books\n",
    "all_books_full_text = load_books(DATA_DIR)\n",
    "\n",
    "# Create the shorter snippets for embedding\n",
    "book_snippets = preprocess_and_truncate(all_books_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9bc7f1-0781-41bc-9cac-f152ec5a733a",
   "metadata": {},
   "source": [
    "<b> Next, using sentence transformer and numpy, we do the embedding on the books and store the embedding </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab69509-1b63-4daa-9ea3-c145e8a08e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings from disk...\n",
      "############ Embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate or Load Embeddings\n",
    "\n",
    "# Initialize the SentenceTransformer model. Make sure it is running on GPU. Otherwise the task will take extremely long time\n",
    "model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', device=\"cuda\")\n",
    "\n",
    "# Check if embeddings already exist to avoid recomputing\n",
    "if os.path.exists(EMBEDDING_FILE):\n",
    "    print(\"Loading existing embeddings from disk...\")\n",
    "    with open(EMBEDDING_FILE, 'rb') as f:\n",
    "        book_embeddings = pickle.load(f)\n",
    "    print(\"############ Embeddings loaded.\")\n",
    "else:\n",
    "    print(\"Generating new embeddings for book snippets...\")\n",
    "    # Get the list of filenames and their corresponding text snippets\n",
    "    filenames = list(book_snippets.keys())\n",
    "    snippets_to_embed = list(book_snippets.values())\n",
    "\n",
    "    # Generate embeddings in one go. This will be fast!\n",
    "    embeddings = model.encode(\n",
    "        snippets_to_embed,\n",
    "        batch_size=8,  # Can use a larger batch size if you have more memory on GPU\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Map filenames back to their embeddings\n",
    "    book_embeddings = {fn: emb for fn, emb in zip(filenames, embeddings)}\n",
    "\n",
    "    # Save the embeddings to a file for future use\n",
    "    with open(EMBEDDING_FILE, 'wb') as f:\n",
    "        pickle.dump(book_embeddings, f)\n",
    "    print(f\"############# Embeddings computed and saved to '{EMBEDDING_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a16478-ea5e-475e-a67c-42e373c20d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your search query (or type 'exit' to quit):  book about fictional wars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---  Top Search Results ---\n",
      "\n",
      "1. Filename: book_3009.txt\n",
      "   Similarity Score: 0.5323\n",
      "   ----------------------------------------\n",
      "   Preview of the book's beginning:\n",
      "\n",
      "Truth and the Myth, by A.R. Narayanan\n",
      "(C) Copyright 2000 by A.R. Narayanan\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "2. Filename: book_2600.txt\n",
      "   Similarity Score: 0.5082\n",
      "   ----------------------------------------\n",
      "   Preview of the book's beginning:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WAR AND PEACE\n",
      "\n",
      "\n",
      "By Leo Tolstoy/Tolstoi\n",
      "\n",
      "\n",
      "    Contents\n",
      "\n",
      "    BOOK ONE: 1805\n",
      "\n",
      "    CHAPTER I\n",
      "\n",
      "    CHAPTER II\n",
      "\n",
      "    CHAPTER III\n",
      "\n",
      "    CHAPTER IV\n",
      "\n",
      "    CHAPTER V\n",
      "\n",
      "    CHAPTER VI\n",
      "\n",
      "    CHAPTER VII\n",
      "\n",
      "    CHAPTER VIII\n",
      "\n",
      "============================================================\n",
      "\n",
      "3. Filename: book_1324.txt\n",
      "   Similarity Score: 0.4959\n",
      "   ----------------------------------------\n",
      "   Preview of the book's beginning:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This Etext prepared by Joseph Gallanar\n",
      "Gallanar@microserve.net\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RUSSIA IN 1919\n",
      "BY ARTHUR RANSOME\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PUBLISHER'S NOTE\n",
      "\n",
      "\n",
      "\n",
      "On August 27, 1914, in London, I made this note in a\n",
      "memorandum book: \"Met Arthur Ransome at_____'s;\n",
      "discussed a book on the Russian's relation to the war in the\n",
      "light of psychological background--folklore.\" The book was\n",
      "not written but the idea that instinctively came to him\n",
      "pervades his every utterance on things Russian.\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "4. Filename: book_2308.txt\n",
      "   Similarity Score: 0.4814\n",
      "   ----------------------------------------\n",
      "   Preview of the book's beginning:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transcribed from the 1895 Oliphant, Anderson and Ferrier edition by David\n",
      "Price, email ccx074@coventry.ac.uk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BUNYAN CHARACTERS--THIRD SERIES\n",
      "Lectures Delivered in St. George's Free Church Edinburgh\n",
      "By Alexander Whyte, D.D.\n",
      "\n",
      "\n",
      "CHAPTER I--THE BOOK\n",
      "\n",
      "\n",
      "   '--the book of the wars of the Lord.'--_Moses_.\n",
      "\n",
      "John Bunyan's _Holy War_ was first published in 1682, six years before\n",
      "its illustrious author's death.  Bunyan wrote this great book when he was\n",
      "still in all the fulness of his intellectual power and in all the\n",
      "ripeness of his spiritual experience.  The _Holy War_ is not the\n",
      "_Pilgrim's Progress_--there is only one _Pilgrim's Progress_.  At the\n",
      "same time, we have Lord Macaulay's word for it that if the _Pilgrim's\n",
      "Progress_ did not exist the _Holy War_ would be the best allegory that\n",
      "============================================================\n",
      "\n",
      "5. Filename: book_2616.txt\n",
      "   Similarity Score: 0.4711\n",
      "   ----------------------------------------\n",
      "   Preview of the book's beginning:\n",
      "\n",
      "\n",
      "\n",
      "MEMOIRS OF GENERAL W. T. SHERMAN\n",
      "\n",
      "By William T. Sherman\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GENERAL W. T. SHERMAN\n",
      "\n",
      "HIS COMRADES IN ARMS,\n",
      "\n",
      "VOLUNTEERS AND REGULARS.\n",
      "\n",
      "Nearly ten years have passed since the close of the civil war in\n",
      "America, and yet no satisfactory history thereof is accessible to\n",
      "the public; nor should any be attempted until the Government has\n",
      "published, and placed within the reach of students, the abundant\n",
      "materials that are buried in the War Department at Washington.\n",
      "These are in process of compilation; but, at the rate of progress\n",
      "for the past ten years, it is probable that a new century will come\n",
      "before they are published and circulated, with full indexes to\n",
      "enable the historian to make a judicious selection of materials.\n",
      "\n",
      "What is now offered is not designed as a history of the war, or\n",
      "even as a complete account of all the incidents in which the writer\n",
      "bore a part, but merely his recollection of events, corrected by a\n",
      "reference to his own memoranda, which may assist the future\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your search query (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define Functions for Semantic Search and Display\n",
    "\n",
    "def find_top_n_books(query, book_embeddings, n=5):\n",
    "    \"\"\"Finds the top N most relevant books for a given query.\"\"\"\n",
    "    # Embed the user's query\n",
    "    query_embedding = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # Reshape for cosine similarity calculation\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and all book snippets\n",
    "    similarities = {}\n",
    "    for filename, book_embedding in book_embeddings.items():\n",
    "        book_embedding = book_embedding.reshape(1, -1)\n",
    "        similarity = cosine_similarity(query_embedding, book_embedding)[0][0]\n",
    "        similarities[filename] = similarity\n",
    "\n",
    "    # Sort the books by similarity score in descending order\n",
    "    sorted_books = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Return the top N books\n",
    "    return sorted_books[:n]\n",
    "\n",
    "def print_top_books(book_list, original_books_text):\n",
    "    \"\"\"Prints the search results nicely, showing the start of the original book.\"\"\"\n",
    "    print(\"\\n\\n---  Top Search Results ---\")\n",
    "    for i, (filename, score) in enumerate(book_list):\n",
    "        # Get the first 30 lines of the original, unprocessed book for context\n",
    "        book_content = original_books_text[filename]\n",
    "        first_30_lines = '\\n'.join(book_content.splitlines()[:30])\n",
    "        \n",
    "        print(f\"\\n{i+1}. Filename: {filename}\")\n",
    "        print(f\"   Similarity Score: {score:.4f}\")\n",
    "        print(\"   \" + \"-\" * 40)\n",
    "        print(\"   Preview of the book's beginning:\")\n",
    "        print(first_30_lines)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Step 6: Run the Interactive Search 🔍\n",
    "\n",
    "# This loop allows for multiple searches without rerunning the script\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your search query (or type 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Find the top 5 most relevant books\n",
    "    top_5_books = find_top_n_books(user_query, book_embeddings, n=5)\n",
    "    \n",
    "    # Print the results using the original, clean text for better readability\n",
    "    print_top_books(top_5_books, all_books_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2c5db-2702-47d8-96d5-f4d8923f01f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
