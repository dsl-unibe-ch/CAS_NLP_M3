{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SFDHxitoA-5"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EQh61dvDCp0"
      },
      "outputs": [],
      "source": [
        "!pip install nltk spacy scikit-learn gensim matplotlib seaborn pandas tqdm flair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkk3qvativ-g"
      },
      "source": [
        "# Wed morning (NN on word frequency)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "4V6BPm9LRNJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBvE96rmDfGh"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "4gyxt4a_eXpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data: inspection and loading"
      ],
      "metadata": {
        "id": "A_4JM1eMRZBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First - open the file and check it. e.g. in a notepad++"
      ],
      "metadata": {
        "id": "Oe0AnK5JYIYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cp \"/content/drive/MyDrive/NLP_M3_data/spambase.csv\" ."
      ],
      "metadata": {
        "id": "64kIxIxrTmzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cat 'spambase.csv'"
      ],
      "metadata": {
        "id": "Ar-8q6JHYPJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('spambase.csv')"
      ],
      "metadata": {
        "id": "PMlUnncjSgXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
        "      Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
        "\n",
        "  (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
        "  \n",
        "  (c) Generated: June-July 1999\n"
      ],
      "metadata": {
        "id": "qjFGax9gUz2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "WLjQLuIIS0Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This table contains information about email texts:\n",
        "\n",
        "The last column of 'spambase.data' denotes whether the e-mail was\n",
        "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
        "Most of the attributes indicate whether a particular word or\n",
        "character was frequently occuring in the e-mail.  The run-length\n",
        "attributes (55-57) measure the length of sequences of consecutive\n",
        "capital letters.  For the statistical measures of each attribute,\n",
        "see the end of this file.  Here are the definitions of the attributes:\n",
        "\n",
        "48 continuous real [0,100] attributes of type word_freq_WORD\n",
        "= percentage of words in the e-mail that match WORD,\n",
        "i.e. 100 * (number of times the WORD appears in the e-mail) /\n",
        "total number of words in e-mail.  A \"word\" in this case is any\n",
        "string of alphanumeric characters bounded by non-alphanumeric\n",
        "characters or end-of-string.\n",
        "\n",
        "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
        "= percentage of characters in the e-mail that match CHAR,\n",
        "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
        "\n",
        "1 continuous real [1,...] attribute of type capital_run_length_average\n",
        "= average length of uninterrupted sequences of capital letters\n",
        "\n",
        "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
        "= length of longest uninterrupted sequence of capital letters\n",
        "\n",
        "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
        "= sum of length of uninterrupted sequences of capital letters\n",
        "= total number of capital letters in the e-mail\n",
        "\n",
        "1 nominal {0,1} class attribute of type spam\n",
        "= denotes whether the e-mail was considered spam (1) or not (0),\n",
        "i.e. unsolicited commercial e-mail.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tGLcHtaHUnCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(df.spam==1), sum(df.spam==0)"
      ],
      "metadata": {
        "id": "BgX6r0b3Uqma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall variable statistics:"
      ],
      "metadata": {
        "id": "6lFB1hJ0V7AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "QIBvFPIZUzXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always check if your data has missing values:"
      ],
      "metadata": {
        "id": "6RymL4Q1WHZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().max()"
      ],
      "metadata": {
        "id": "ne8tMTTpVeHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data preparation"
      ],
      "metadata": {
        "id": "F-GBDk-EgBUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first to split the table into independent variables, and target variable."
      ],
      "metadata": {
        "id": "PhOF5O70aiPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop('spam', axis=1).values.astype(np.float32)\n",
        "y = df['spam'].values.astype(np.int64)\n",
        "column_names = list(df.columns)"
      ],
      "metadata": {
        "id": "TahV_VoYbIok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, x.dtype"
      ],
      "metadata": {
        "id": "4F4VDQVFbrb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape, y.dtype"
      ],
      "metadata": {
        "id": "wanWyWg4cG1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand the input feature distributions.\n",
        "We could make a histogram for each value, but this is exactly what violin plot does for multiple variables.\n",
        "\n",
        "e.g. for first two columns:"
      ],
      "metadata": {
        "id": "iF0isCUbj8d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names[:2]"
      ],
      "metadata": {
        "id": "zWSM_PMAYN_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(x[:, 0], 100);"
      ],
      "metadata": {
        "id": "VK0QCrZYacmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_col = 1\n",
        "df_x = pd.DataFrame(x[:, :n_col], columns=column_names[:n_col])\n",
        "\n",
        "plt.figure(figsize=(15, 4)) # always make plots big enough so you see there anything\n",
        "sns.violinplot(data=df_x, cut=0)\n",
        "\n",
        "plt.xticks(rotation=45, horizontalalignment='right'); # make text visible: @ 45 degree + aligned rihght"
      ],
      "metadata": {
        "id": "5cU582wjZQv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now for all colums:"
      ],
      "metadata": {
        "id": "FWmoWyd9as3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = pd.DataFrame(x, columns=column_names[:-1])\n",
        "\n",
        "plt.figure(figsize=(15, 4)) # always make plots big enough so you see there anything\n",
        "sns.violinplot(data=df_x, cut=0)\n",
        "\n",
        "plt.xticks(rotation=45, horizontalalignment='right'); # make text visible: @ 45 degree + aligned rihght"
      ],
      "metadata": {
        "id": "81XsOo0He2am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that range of values is very different for different columns. This can negatively impact the training of neural networs.\n",
        "\n",
        "It is always a good practice to standardise inputs (and continous outputs)\n",
        "\n",
        "To make distribution of all features similar - we standardize them:"
      ],
      "metadata": {
        "id": "CsawDtVjjzDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_s = StandardScaler().fit_transform(x)"
      ],
      "metadata": {
        "id": "ZvbMTSznePu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = pd.DataFrame(x_s, columns=column_names[:-1])\n",
        "\n",
        "plt.figure(figsize=(25, 4))\n",
        "sns.violinplot(data=df_x, cut=0)\n",
        "\n",
        "plt.xticks(rotation=45, horizontalalignment='right');"
      ],
      "metadata": {
        "id": "HQuFXEdBfxw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally to be able to chech the performace of out models - we will split data into independent training and validation sets:"
      ],
      "metadata": {
        "id": "rESV72bMcP_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_validation, y_train, y_validation = train_test_split(x_s, y, shuffle=True, stratify=y, test_size=0.2)"
      ],
      "metadata": {
        "id": "Jv7frw1vY886"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, x_validation.shape, y_train.shape, y_validation.shape"
      ],
      "metadata": {
        "id": "aTNK0UfBZvlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data loading"
      ],
      "metadata": {
        "id": "kRTNzxvNRcjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform model trainign on the GPU if it is available. This means we will:\n",
        "1. place model, inputs and trget data on the `device`\n",
        "2. afterwards move the resulting calculations performed on device back to 'host' - memory directly accessible by CPU"
      ],
      "metadata": {
        "id": "qTurN1J7bVBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')  # use first available GPU\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ],
      "metadata": {
        "id": "MMZNNSiengH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_device()"
      ],
      "metadata": {
        "id": "HZ9Ac3cuniAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert numpy arrays into torch tensors"
      ],
      "metadata": {
        "id": "Q5MciYPmnviz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tra_tensor =  torch.tensor(x_train).to(device)\n",
        "y_tra_tensor =  torch.tensor(y_train).to(device)\n",
        "x_val_tensor =  torch.tensor(x_validation).to(device)\n",
        "y_val_tensor =  torch.tensor(y_validation).to(device)"
      ],
      "metadata": {
        "id": "cPwFDxeIi1aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "4KuAJKUVlIjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tra_tensor"
      ],
      "metadata": {
        "id": "a2u_yeQOlKW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for model training - we will group the independent variables (model input) ad the dependent (target, desired model ouput) together"
      ],
      "metadata": {
        "id": "5LZTjwmvlpt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tra_tensor = data_utils.TensorDataset(x_tra_tensor, y_tra_tensor)\n",
        "val_tensor = data_utils.TensorDataset(x_val_tensor, y_val_tensor)"
      ],
      "metadata": {
        "id": "9-G3ndk7jLVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_tensor"
      ],
      "metadata": {
        "id": "9cHCeOXIl6jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_tensor.tensors"
      ],
      "metadata": {
        "id": "Ey38NeQ6lZ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stochastic gradient descent optimizations are performed sequentially on parts of the training algoritms - (mini)batches."
      ],
      "metadata": {
        "id": "aRUfB-eGcv3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100"
      ],
      "metadata": {
        "id": "6k3SvjicmImO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DataLoader` is a utility class that helps with efficiently loading and iterating over large datasets for the training:"
      ],
      "metadata": {
        "id": "5USosF_6fFkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tra_loader = data_utils.DataLoader(dataset=tra_tensor, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=True, drop_last=True,\n",
        "                                   #num_workers=2, persistent_workers=True  # <- good for produciton\n",
        "                                   )\n",
        "\n",
        "val_loader = data_utils.DataLoader(dataset=val_tensor, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=False, drop_last=False,\n",
        "                                   #num_workers=2, persistent_workers=True  # <- good for produciton\n",
        "                                   )"
      ],
      "metadata": {
        "id": "flTGvIJEmBpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, s in enumerate(tra_loader):\n",
        "  print(i, s)"
      ],
      "metadata": {
        "id": "fWNPZopPodwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, s in enumerate(tra_loader):\n",
        "  print(i, s[0].shape, s[1].shape)"
      ],
      "metadata": {
        "id": "h9i8e7iun2Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('training size after batching:', len(tra_loader) * tra_loader.batch_size)\n",
        "print('training dataset size:', len(x_tra_tensor))"
      ],
      "metadata": {
        "id": "v1V979ROoqRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model"
      ],
      "metadata": {
        "id": "rqwx5y_dRii4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When designing your model first thing to think about - is waht are you putting in and what do you want out.\n",
        "Think of it in term of:\n",
        "- shape\n",
        "- type\n",
        "- value range\n",
        "\n",
        "these would define:\n",
        "- ?\n",
        "- ?"
      ],
      "metadata": {
        "id": "aIsehjc2facQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_s.shape, y.shape"
      ],
      "metadata": {
        "id": "tcDEDWvmqH4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamClassificationModel_1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SpamClassificationModel_1, self).__init__()\n",
        "\n",
        "    # single layer model\n",
        "    self.l1 = nn.Linear(57, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = x\n",
        "    #print('input: ', y.shape)\n",
        "\n",
        "    # output - just logits. for optimized trainign - without activaiton here\n",
        "    y = self.l1(y)  # returns logits\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "7sijNTmppv4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_scm1 = SpamClassificationModel_1().to(device)"
      ],
      "metadata": {
        "id": "DrkbHf52rEM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_tensor.shape"
      ],
      "metadata": {
        "id": "PZXUYsVurN_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass of validation set through the model, to see output distribution\n",
        "pred = model_scm1(x_val_tensor)"
      ],
      "metadata": {
        "id": "__Eb_6FCrVOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "id": "XfeetP8Crdqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_proba = F.softmax(pred.detach(), dim=1)\n",
        "\n",
        "# to get value from a tensor - we copy it to CPU-accessible memory,\n",
        "# then convert to a numpy array\n",
        "pred_proba_value = pred_proba.cpu().numpy()"
      ],
      "metadata": {
        "id": "TCPp77BusVkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_proba_value.shape"
      ],
      "metadata": {
        "id": "SAIkMurcXyDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brobability belonging to class 1 - spam:\n",
        "class1_proba_value = pred_proba_value[:, 1]\n",
        "\n",
        "# plot histogram\n",
        "plt.hist(class1_proba_value, 100);"
      ],
      "metadata": {
        "id": "FtvnAhwxsc8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_validation, class1_proba_value)"
      ],
      "metadata": {
        "id": "uMo2cGo5sqV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Trainig"
      ],
      "metadata": {
        "id": "RoW-n86KRijC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function (Cross Entropy Loss)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Stochastic Gradient Descent)\n",
        "optimizer = optim.SGD(model_scm1.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "Wa5vju86rRP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs for training\n",
        "N_EPOCHS = 1000"
      ],
      "metadata": {
        "id": "9qp7Sw5l_6-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store training and validation loss and accuracy\n",
        "tra_loss = []\n",
        "val_loss = []\n",
        "tra_acc = []\n",
        "val_acc = []\n",
        "\n",
        "# Calculate the total number of training and validation samples\n",
        "n_tra = len(tra_loader) * tra_loader.batch_size\n",
        "n_val = len(x_validation)\n",
        "\n",
        "# Iterate over epochs\n",
        "for epoch in tqdm.tqdm_notebook(range(N_EPOCHS)):  # loop over the dataset `N_EPOCHS` times\n",
        "    model_scm1.train()  # Set the model to training mode\n",
        "    # 1. Training\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    for i, data in enumerate(tra_loader):\n",
        "\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model_scm1(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).detach().cpu().numpy()\n",
        "\n",
        "    tra_loss.append(epoch_loss / (i + 1))\n",
        "    tra_acc.append(epoch_correct / n_tra)\n",
        "\n",
        "    model_scm1.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # 2. Validation\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    for i, data in enumerate(val_loader):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        with torch.no_grad(): # disable gradient calculation during evaluation\n",
        "            outputs = model_scm1(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).detach().cpu().numpy()\n",
        "\n",
        "    val_loss.append(epoch_loss / (i + 1))\n",
        "    val_acc.append(epoch_correct / n_val)\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "zoHDmW0T_0RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss and accuracy evolution\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].plot(tra_loss, label='training loss')\n",
        "ax[0].plot(val_loss, label='validation loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(tra_acc, label='training accuracy')\n",
        "ax[1].plot(val_acc, label='validation accuracy')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "rTnB_B6qKNKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you see?"
      ],
      "metadata": {
        "id": "X8rBjL9BYure"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets inspect the model's performance differently"
      ],
      "metadata": {
        "id": "3LmIvfNlZXzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model_scm1(x_val_tensor)"
      ],
      "metadata": {
        "id": "ksXz4aYKXJQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_value = F.softmax(pred.detach(), dim=1).cpu().numpy()"
      ],
      "metadata": {
        "id": "yscA1YI3XJQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(pred_value[:, 1], 100);"
      ],
      "metadata": {
        "id": "Ye6-vYr9XJQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_validation, pred_value.argmax(axis=1))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kR83b_CNXJQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you see?"
      ],
      "metadata": {
        "id": "KRecX78zZ3R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model inspection"
      ],
      "metadata": {
        "id": "_D-dicgbRsin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will make use of the Tensorboard to visualize the training evolution and the model's architecture"
      ],
      "metadata": {
        "id": "9NWrZAu0Z65c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rm -fr runs"
      ],
      "metadata": {
        "id": "uqND3CzDgPBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SummaryWriter for TensorBoard logging, specifying the directory 'runs/model_scm1' for storing logs\n",
        "writer = SummaryWriter('runs/model_scm1')\n",
        "\n",
        "# Define a layout for custom scalars in TensorBoard\n",
        "layout = {\n",
        "    \"LA\": {\n",
        "        \"loss\": [\"Multiline\", [\"loss/training\", \"loss/validation\"]],\n",
        "        \"accuracy\": [\"Multiline\", [\"accuracy/training\", \"accuracy/validation\"]],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Add the custom scalars layout to TensorBoard\n",
        "writer.add_custom_scalars(layout)\n",
        "\n",
        "# Create an instance of the SpamClassificationModel_1 and move it to the selected device (e.g., GPU if available)\n",
        "model_scm1 = SpamClassificationModel_1().to(device)\n",
        "\n",
        "# Get a batch of data samples and labels from the training DataLoader\n",
        "xs, ys = next(iter(tra_loader))\n",
        "\n",
        "# Add the model's computation graph to TensorBoard, visualizing the model's architecture\n",
        "writer.add_graph(model_scm1, xs)"
      ],
      "metadata": {
        "id": "3YPDjX3XZc4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function (Cross Entropy Loss)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam optimizer)\n",
        "# - model_scm1.parameters(): Specifies the model's parameters that need optimization\n",
        "# - lr=0.001: Learning rate determines the step size for parameter updates during training\n",
        "optimizer = optim.Adam(model_scm1.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "IUFOLH8MaTJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 2000"
      ],
      "metadata": {
        "id": "xQbMVnb6aTJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of training and validation samples\n",
        "n_tra = len(tra_loader) * tra_loader.batch_size  # Total training samples\n",
        "n_val = len(x_validation)  # Total validation samples\n",
        "\n",
        "# Iterate over the specified number of training epochs\n",
        "for epoch in tqdm.tqdm_notebook(range(N_EPOCHS)):  # Loop over the dataset multiple times\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model_scm1.train()\n",
        "\n",
        "    # 1. Training\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "\n",
        "    # Iterate over batches of training data\n",
        "    for i, data in enumerate(tra_loader):\n",
        "\n",
        "        # Get the inputs and labels from the batch; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the gradients of the model parameters (reset gradients)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute model predictions for the inputs\n",
        "        outputs = model_scm1(inputs)\n",
        "\n",
        "        # Compute the loss between model predictions and actual labels\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass: Compute gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using the optimizer (perform one optimization step)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss and calculate the number of correctly classified samples in this batch\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).detach().cpu().numpy()\n",
        "\n",
        "    # Log training loss and accuracy for this epoch using TensorBoard\n",
        "    writer.add_scalar('loss/training', epoch_loss / (i + 1), epoch)\n",
        "    writer.add_scalar('accuracy/training', epoch_correct / n_tra, epoch)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model_scm1.eval()\n",
        "\n",
        "    # 2. Validation\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "\n",
        "    # Iterate over batches of validation data\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        # Get the inputs and labels from the batch; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Perform forward pass without gradient computation (no gradient updates)\n",
        "        with torch.no_grad():\n",
        "            outputs = model_scm1(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Accumulate the loss and calculate the number of correctly classified samples in this batch\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).detach().cpu().numpy()\n",
        "\n",
        "    # Log validation loss and accuracy for this epoch using TensorBoard\n",
        "    writer.add_scalar('loss/validation', epoch_loss / (i + 1), epoch)\n",
        "    writer.add_scalar('accuracy/validation', epoch_correct / n_val, epoch)\n",
        "\n",
        "# Training loop is finished\n",
        "print('Finished Training')\n",
        "\n",
        "# Close the TensorBoard writer to save the logs\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "hG3R0PVTaTJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run tensorboard to visualize data:"
      ],
      "metadata": {
        "id": "qCu6cw5VduAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "AoYqFMT4czgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVkL_QWNd9Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Exercise 1."
      ],
      "metadata": {
        "id": "x0W2XZkIdyjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use model with 2 layers\n",
        "Evaluate effect of the hyperparameters on model performance:\n",
        "- learnign rate (1.e-5, 3.e-5, 1.e-4, 3.e-4, 1.e-3, 3.e-3, 1.e-3, 3.e-3, 1.e-2, 3.e-2, 1.e-1, 3.e-1)\n",
        "- batch size (2, 4, 8, 16, 32, 64, 128, 256, 512)\n",
        "\n",
        "fill the table from several runs and plot the statistics. divide in groups to work on one task.\n",
        "Dicuss in your group additional metrics you could evaluate and add them to the table."
      ],
      "metadata": {
        "id": "-XXCDkmkpcDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'lr': [],\n",
        "    'batch_size': [],\n",
        "    'best_val_accuracy': [],\n",
        "    'best_val_acc_epoch': []\n",
        "}"
      ],
      "metadata": {
        "id": "GuhTDH2BfGd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot runs comparisson. e.g. for the learning rate:\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].plot(results['lr'], results['best_val_accuracy'], '*')\n",
        "\n",
        "ax[1].plot(results['lr'], results['best_val_acc_epoch'], '*')\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "BW5_8H92uWAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Exercise 2."
      ],
      "metadata": {
        "id": "PzNn1ybJf2TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore different model architectures: number of layers, number of features (neurons) per layer, etc\n",
        "\n",
        "Fill a dictionaly sililar to exercise 1, and compare the results of different models."
      ],
      "metadata": {
        "id": "SJEVVX6Tf9H8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1nUMIbUoLl-"
      },
      "source": [
        "# Wed evening (NN on tf-idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session we will continue with building simple neural networks.\n",
        "We will use the more sophisticated features, and rely on previously established intuition about building the NNs."
      ],
      "metadata": {
        "id": "mND6VjdqgoWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "lwxYyCso5IFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zoQhDMqOrtHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fetch dataset"
      ],
      "metadata": {
        "id": "2wajOn8m5Oq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_20newsgroups().target_names"
      ],
      "metadata": {
        "id": "D5xybPcrrcfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
        "\n",
        "We strip the headers and footers, as those can make the task easier."
      ],
      "metadata": {
        "id": "NL8T5Kb3r-nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
        "SEED = 64\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "L_1c1ZY5sDwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data structure\n",
        "train_data.keys()"
      ],
      "metadata": {
        "id": "LLNn0H4FsPBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data input elements\n",
        "train_data.data[:3]"
      ],
      "metadata": {
        "id": "Gwj8tT7usQyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocess and inspect dataset"
      ],
      "metadata": {
        "id": "rP4-PTgK5jbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the train_data and test_data\n",
        "for d in [train_data, test_data]:\n",
        "\n",
        "  # Remove leading and trailing whitespace, tab,\n",
        "  # and new line characters from each data point\n",
        "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
        "\n",
        "  # Get the indices of data points with non-empty content\n",
        "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
        "\n",
        "  # Filter the data and target lists to keep only non-empty data points\n",
        "  d.data = [d.data[i] for i in ok_idx]\n",
        "  d.target = [d.target[i] for i in ok_idx]"
      ],
      "metadata": {
        "id": "8KOXWnZ-vH2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data labels elements\n",
        "train_data.target"
      ],
      "metadata": {
        "id": "kmPukYChsebf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data.data), len(test_data.data)"
      ],
      "metadata": {
        "id": "3wtbKp2csr-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_data.data[0]\n",
        "sample_label = train_data.target[0]\n"
      ],
      "metadata": {
        "id": "hNJ87Mmksy3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample)\n",
        "print(f'label={sample_label}, ({categories[sample_label]})')"
      ],
      "metadata": {
        "id": "aveCoShCs0wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ],
      "metadata": {
        "id": "Z0floa6Fxw_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data to TF-IDF vectors, max 10 words\n",
        "vectorizer = TfidfVectorizer(max_features=10)\n",
        "\n",
        "# `object.fit` followed by processing by `object.transform` or the\n",
        "# joined `object.fit_transform` is the\n",
        "train_vectors = vectorizer.fit_transform(train_data.data[:3])"
      ],
      "metadata": {
        "id": "VUz-3pSW0C5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(len(feature_names))"
      ],
      "metadata": {
        "id": "h9yGUw5L0dm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the vectors are stored in sparse format:\n",
        "train_vectors"
      ],
      "metadata": {
        "id": "_BaP1Jz43RR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get the dense representation we need to convert them to an array:\n",
        "train_vectors.toarray()"
      ],
      "metadata": {
        "id": "a_0wi9r33wkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 3))\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[0])\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[1])\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[2])\n",
        "plt.xticks(rotation=45, horizontalalignment='right');\n",
        "plt.ylim(0,1);"
      ],
      "metadata": {
        "id": "QZaVyRRK0T7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data to TF-IDF vectors, all words\n",
        "vectorizer = TfidfVectorizer(max_features=None)\n",
        "train_vectors = vectorizer.fit_transform(train_data.data)\n",
        "test_vectors = vectorizer.transform(test_data.data)"
      ],
      "metadata": {
        "id": "Jf4GAKPHtZSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "af = train_vectors.toarray().flatten()\n",
        "# plot only present words (tfidf>0)\n",
        "aff = af[af>0]\n",
        "plt.hist(aff, 100);"
      ],
      "metadata": {
        "id": "IQTU4UaUuawJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create and train the model"
      ],
      "metadata": {
        "id": "I4Zi-tyR5tNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oyrbUfDpqQX"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model hyperparameters\n",
        "input_dim = train_vectors.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = len(categories)\n",
        "\n",
        "# Create the model instance\n",
        "model = Classifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model and data to the device\n",
        "model = model.to(device)\n",
        "train_vectors_t = torch.tensor(train_vectors.toarray(), dtype=torch.float).to(device)\n",
        "train_labels_t = torch.tensor(train_data.target, dtype=torch.long).to(device)\n",
        "test_vectors_t = torch.tensor(test_vectors.toarray(), dtype=torch.float).to(device)\n",
        "test_labels_t = torch.tensor(test_data.target, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "def train(model, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(train_vectors_t)\n",
        "    loss = criterion(predictions, train_labels_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_vectors_t)\n",
        "        predicted_labels = torch.argmax(predictions, dim=1)\n",
        "        accuracy = torch.sum(predicted_labels == test_labels_t).item() / len(test_labels_t)\n",
        "    return accuracy\n",
        "\n",
        "# Training and evaluation loop\n",
        "NUM_EPOCHS = 100\n",
        "accuracy_arr = []\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "    train(model, optimizer, criterion)\n",
        "    accuracy = evaluate(model)\n",
        "    #print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    accuracy_arr.append(accuracy)\n",
        "\n",
        "plt.plot(accuracy_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Exercise"
      ],
      "metadata": {
        "id": "G-NUgYrD6Dfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work in 3 groups:\n",
        " - present final group results in the end (5 min per group):\n",
        " - half time through - share code of your intermediate results (on Zoom) so that other groups use it for final results:\n",
        "\n",
        "1. Optimize the model architecture to improve performance.\n",
        "2. Study dependence of performance & training time on number of tf-idf features\n",
        "3. Study relevant performance metrics. Make evaluation code for both training and the test sets."
      ],
      "metadata": {
        "id": "RO89XTV16grj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtwfrYsDoKVX"
      },
      "source": [
        "# Thu morning RNN on embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports/utils"
      ],
      "metadata": {
        "id": "wj0VGjy38y_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as gd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "U_lEH0fEE5Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import tqdm\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vinmxOLyyTS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "nI-xF3SBy9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pckl(file_name, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_pckl(d, file_name, pr=None, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
      ],
      "metadata": {
        "id": "m1DUCo_g2x8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(input_string):\n",
        "    # Use regular expression to remove all punctuation characters\n",
        "    return re.sub(r'[^\\w\\s]', '', input_string)  # everything which is not a word (\\w) or space (\\s) -> empty string ('')"
      ],
      "metadata": {
        "id": "4PpKyLF7F9Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and preprocess the dataset"
      ],
      "metadata": {
        "id": "XX6Kx_zwBb8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
        "SEED = 64\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "ZW6zkt3LB47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the train_data and test_data\n",
        "for d in [train_data, test_data]:\n",
        "\n",
        "  # Remove leading and trailing whitespace, tab,\n",
        "  # and new line characters from each data point\n",
        "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
        "\n",
        "  # Get the indices of data points with non-empty content\n",
        "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
        "\n",
        "  # Filter the data and target lists to keep only non-empty data points\n",
        "  d.data = [d.data[i] for i in ok_idx]\n",
        "  d.target = [d.target[i] for i in ok_idx]"
      ],
      "metadata": {
        "id": "7HsPnlUEB47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explore Word2Vec (Homework)"
      ],
      "metadata": {
        "id": "EV15Ei_70NH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the tiny dataset example explore similar words and word arythmetic.\n",
        "\n",
        "Use your imagination, e.g. snake-long+short, or brick-hard+soft, etc"
      ],
      "metadata": {
        "id": "sO_Dqnvf_3UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tiny dataset"
      ],
      "metadata": {
        "id": "cAafXxdv_XNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus (replace with your own preprocessing steps)\n",
        "corpus = [\"I love to eat pizza\", \"I hate Mondays\", \"Pizza is delicious\", \"I enjoy playing tennis\"]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, )"
      ],
      "metadata": {
        "id": "74dRLkXQ-iux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"pizza\")\n",
        "print(\"Similar words to 'pizza':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "k9RvdcdY-kxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform word embeddings arithmetic\n",
        "result = model.wv.most_similar(positive=[\"tennis\", \"hate\"], negative=[\"love\"])  # \"tenis\" - \"love\" + \"hate\"\n",
        "print(\"Word embeddings arithmetic:\")\n",
        "for word, similarity in result:\n",
        "    print(word, similarity)\n"
      ],
      "metadata": {
        "id": "SbIZeU2e-qkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see - on tiny dataset the vectors don't make much sense.\n",
        "You are encouraged to explore the embedding vectors built based on the bigger datasets in the nxt 2 sections"
      ],
      "metadata": {
        "id": "3rQIG8CgBG5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bigger dataset"
      ],
      "metadata": {
        "id": "oC5fQSXhJwMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus (replace with your own preprocessing steps)\n",
        "corpus = train_data.data\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [remove_punctuation(sentence.lower()).split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized_corpus, vector_size=300, window=5, min_count=1, workers=4, )"
      ],
      "metadata": {
        "id": "nCwp9tAdE3m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"network\"\n",
        "similar_words = model.wv.most_similar(word, topn=20)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "kRjoibFZFBsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try removing the stopwords and re-fitting the model (see sections below hoe to do it)\n",
        "\n"
      ],
      "metadata": {
        "id": "tqACxydJCsKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 300k words"
      ],
      "metadata": {
        "id": "9JMEoreu_hly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here a pretrained model on many news articles is downloaded (Downloading it can take a while!). Feature vectors are of length 300, total 300k words."
      ],
      "metadata": {
        "id": "YoP9nc63DKUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the word2vec-google-news-300 model\n",
        "model_gn300 = gd.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "RhiLvBooJzcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gn300.vectors.shape"
      ],
      "metadata": {
        "id": "9kilDS5znQ2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"car\"\n",
        "similar_words = model_gn300.most_similar(word)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "ZI_Dau6jJ254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"god\"\n",
        "similar_words = model_gn300.most_similar(word)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "IklZGfZ1Keq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model_gn300.most_similar(\"python\")\n",
        "print(\"Similar words to 'pizza':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)\n",
        "\n",
        "# Perform word embeddings arithmetic\n",
        "result = model_gn300.most_similar(positive=[\"python\", \"young\"], negative=[\"old\"])\n",
        "print(\"Word embeddings arithmetic:\")\n",
        "for word, similarity in result:\n",
        "    print(word, similarity)\n",
        "\n",
        "# Visualize word embeddings using t-SNE (replace with your own visualization code)\n",
        "# ...\n",
        "\n",
        "# Additional tasks and experiments with word embeddings\n",
        "# ...\n"
      ],
      "metadata": {
        "id": "taBzRcUYEqSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preparing embedding dataset (not run in the class - time-consuming)"
      ],
      "metadata": {
        "id": "sHqmaxTw7Myf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Embedding utilities"
      ],
      "metadata": {
        "id": "apoYWHguBV9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create word embeddigns with Word2Vec"
      ],
      "metadata": {
        "id": "-BHATKtd8xc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download (takes a while!) the word2vec-google-news-300 model\n",
        "model_gn300 = gd.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "Z4THcnUGGahs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vec(model, word):\n",
        "  # convert word to vector\n",
        "  # Check if the model has an index for the word, of so get vector,\n",
        "  # otherwise return None\n",
        "  return model.get_vector(word) if model.has_index_for(word) else None"
      ],
      "metadata": {
        "id": "pq-cVuxRxMKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert text to word vectors\n",
        "# using a specified word embedding model\n",
        "# (default - word2vec-google-news-300 model, Download takes a while!\n",
        "# you can also try yours)\n",
        "def convert_text_to_vecs(text, model=model_gn300):\n",
        "  # Tokenize the input text into individual words\n",
        "  words = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "  # Get word vectors for each word in the text using the specified word embedding model\n",
        "  wvs = [get_vec(model, word) for word in words]\n",
        "\n",
        "  # Filter out None values (word vectors that couldn't be found in the model)\n",
        "  wvs = [v for v in wvs if v is not None]\n",
        "\n",
        "  # Convert the list of word vectors to a NumPy array\n",
        "  wvs = np.array(wvs)\n",
        "\n",
        "  # Return the array of word vectors for the input text\n",
        "  return wvs"
      ],
      "metadata": {
        "id": "f32YKff7x4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few helper functions to convert input dataset, and to save whole dataset to file"
      ],
      "metadata": {
        "id": "eceVSYzTA6uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_corpus_to_vecs(corpus, convert_corpus_to_vecs_fn):\n",
        "  # for each text in the corpus - vectorize it\n",
        "  # using list comprehension. Use tqdm to display progress\n",
        "  return [convert_corpus_to_vecs_fn(text) for text in tqdm.auto.tqdm(corpus)]"
      ],
      "metadata": {
        "id": "YGUcVK3Fx_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_embedding_dataset(tra_input, tra_labels,\n",
        "                              val_input, val_labels,\n",
        "                              filename,\n",
        "                              convert_text_to_vecs_fn):\n",
        "  # Prepare and save an embedding dataset for training and validation.\n",
        "  print('embedding training data:')\n",
        "  tra_data_vecs = convert_corpus_to_vecs(tra_input, convert_text_to_vecs_fn)\n",
        "\n",
        "  print('embedding validation data:')\n",
        "  val_data_vecs = convert_corpus_to_vecs(val_input, convert_text_to_vecs_fn)\n",
        "\n",
        "  # remove empty elements\n",
        "  ok_idx = [i for i, s in enumerate(tra_data_vecs) if len(s)>0]\n",
        "  tra_data_vecs = [tra_data_vecs[i] for i in ok_idx]\n",
        "  tra_labels = [tra_labels[i] for i in ok_idx]\n",
        "\n",
        "  ok_idx = [i for i, s in enumerate(val_data_vecs) if len(s)>0]\n",
        "  val_data_vecs = [val_data_vecs[i] for i in ok_idx]\n",
        "  val_labels = [val_labels[i] for i in ok_idx]\n",
        "\n",
        "  print(f'preparing and saving dataset to: {filename}')\n",
        "\n",
        "  # Create a dataset dictionary containing training and validation data and labels\n",
        "  dataset = {\n",
        "    'tra_data': tra_data_vecs,\n",
        "    'tra_labels': tra_labels,\n",
        "    'val_data': val_data_vecs,\n",
        "    'val_labels': val_labels,\n",
        "  }\n",
        "\n",
        "  # Save the dataset as a pickle file\n",
        "  save_pckl(dataset, filename, path='./')"
      ],
      "metadata": {
        "id": "EolcP7t417Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe and RoBERTa embeddings (with flair)"
      ],
      "metadata": {
        "id": "6B65X6TPzvgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, TransformerWordEmbeddings\n",
        "from flair.data import Sentence"
      ],
      "metadata": {
        "id": "T-PYAb2tDtKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download models for word embeddings with GloVe and RoBERTa\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "roberta_embedding = TransformerWordEmbeddings('roberta-base')\n",
        "\n",
        "# Create a DocumentPoolEmbeddings object (optional but helpful)\n",
        "document_embeddings_glove = DocumentPoolEmbeddings([glove_embedding])\n",
        "document_embeddings_roberta = DocumentPoolEmbeddings([roberta_embedding])\n"
      ],
      "metadata": {
        "id": "ukDDr4132om0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_embedding(text, doc_embedding_model):\n",
        "  # Create a Flair Sentence object\n",
        "  sentence = Sentence(text)\n",
        "\n",
        "  # Embed the sentence\n",
        "  try:\n",
        "    doc_embedding_model.embed(sentence)\n",
        "  except Exception as e:\n",
        "    # if model can't convert a text - print it, before raising the exception\n",
        "    print('failed text:', text)\n",
        "    raise e\n",
        "\n",
        "  embeddings = [token.embedding.cpu().numpy() for token in sentence]\n",
        "  return np.array(embeddings)"
      ],
      "metadata": {
        "id": "uA60Vu8F1giW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_glove(text):\n",
        "  return convert_text_to_embedding(text, document_embeddings_glove)\n",
        "\n",
        "def convert_text_to_roberta(text):\n",
        "  return convert_text_to_embedding(text, document_embeddings_roberta)\n"
      ],
      "metadata": {
        "id": "aNWc1s7g6BYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create word2vec embeddings"
      ],
      "metadata": {
        "id": "bi1QNGL5BNZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_vecs\n",
        "                          )"
      ],
      "metadata": {
        "id": "gtQxEiXzxxgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat.pckl \"/content/drive/MyDrive/Colab Data/NLP_M3_data\""
      ],
      "metadata": {
        "id": "p8Gw4tVw5yZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create GloVe embeddings"
      ],
      "metadata": {
        "id": "ccbxW2q4BgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat_gl.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_glove\n",
        "                          )"
      ],
      "metadata": {
        "id": "E_nuPY_S1NxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat_gl.pckl \"/content/drive/MyDrive/Colab Data/NLP_M3_data\""
      ],
      "metadata": {
        "id": "KzmZdxTQ6u3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create RoBERTa embeddings (run with a GPU)"
      ],
      "metadata": {
        "id": "CsTsJdNMBlhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat_rb.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_roberta\n",
        "                          )"
      ],
      "metadata": {
        "id": "18U8TklyByWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat_rb.pckl \"/content/drive/MyDrive/Colab Data/NLP_M3_data\""
      ],
      "metadata": {
        "id": "6mqQkRqACkH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Inspect prepared datasets:"
      ],
      "metadata": {
        "id": "_NiyDVri7Wt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_20newsgroups_3_cat = load_pckl('dataset_20newsgroups_3_cat.pckl', '/content/drive/MyDrive/NLP_M3_data')"
      ],
      "metadata": {
        "id": "R-dlLdY6dQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_data = dataset_20newsgroups_3_cat['tra_data']\n",
        "tra_labels = dataset_20newsgroups_3_cat['tra_labels']\n",
        "val_data = dataset_20newsgroups_3_cat['val_data']\n",
        "val_labels = dataset_20newsgroups_3_cat['val_labels']"
      ],
      "metadata": {
        "id": "Wes5HklPddf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tra_data), len(tra_labels), len(val_data), len(val_labels)"
      ],
      "metadata": {
        "id": "vdkC_4W3doQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_data[0].shape"
      ],
      "metadata": {
        "id": "yUY8vO-7d2cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LSTM Training:"
      ],
      "metadata": {
        "id": "CirttWtLCH3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ggt9UedMCXGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pckl(file_name, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_pckl(d, file_name, pr=None, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
      ],
      "metadata": {
        "id": "7mTTIDjjCay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_name = 'dataset_20newsgroups_3_cat.pckl'\n",
        "\n",
        "\n",
        "# Load the embedded 20 Newsgroups dataset\n",
        "dataset = load_pckl(ds_name,\n",
        "                    '/content/drive/MyDrive/NLP_M3_data')\n",
        "tra_data = dataset['tra_data']\n",
        "tra_labels = dataset['tra_labels']\n",
        "val_data = dataset['val_data']\n",
        "val_labels = dataset['val_labels']\n",
        "\n",
        "ok_idx = [i for i, s in enumerate(tra_data) if len(s)>0]\n",
        "tra_data = [tra_data[i] for i in ok_idx]\n",
        "tra_labels = [tra_labels[i] for i in ok_idx]\n",
        "\n",
        "ok_idx = [i for i, s in enumerate(val_data) if len(s)>0]\n",
        "val_data = [val_data[i] for i in ok_idx]\n",
        "val_labels = [val_labels[i] for i in ok_idx]\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Sjh8Uw6GD2fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "\n",
        "# make preprocessing function converting data to torch tensors\n",
        "def preprocess(data, labels):\n",
        "    # Make random crops of the sequences up to max_len length,\n",
        "    # Pad short sequences to ensure consistent sequence length\n",
        "    max_len = 256  # Maximum sequence length\n",
        "    lens = [len(d) for d in data]  # Get the length of each data point\n",
        "\n",
        "    # Calculate random offsets for padding\n",
        "    ofs = [np.random.randint(0, max(1, len_i - max_len)) for len_i in lens]\n",
        "\n",
        "    # Apply padding based on offsets\n",
        "    data = [d[o:o + max_len] for d, o in zip(data, ofs)]\n",
        "    max_len = max([len(d) for d in data])  # Update maximum sequence length after padding\n",
        "\n",
        "    # Pad sequences with zeros to match the maximum sequence length\n",
        "    # pad_width contains size of padding  left and righ in each dimension\n",
        "    data_padded = [np.pad(d, pad_width=((0, max_len - len(d)),\n",
        "                                        (0, 0))) for d in data]\n",
        "\n",
        "    # Convert data and labels to NumPy arrays\n",
        "    data_padded = np.array(data_padded)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Convert NumPy arrays to PyTorch tensors\n",
        "    # and move them to the specified device (e.g., GPU)\n",
        "    data_t = torch.tensor(data_padded, dtype=torch.float32).to(device)\n",
        "    labels_t = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "\n",
        "    return data_t, labels_t\n",
        "\n",
        "# make data loader\n",
        "\n",
        "train_loader = DataLoader(list(zip(tra_data, tra_labels)),\n",
        "                          batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=lambda x: preprocess(*zip(*x)))\n",
        "\n",
        "test_loader = DataLoader(list(zip(val_data, val_labels)),\n",
        "                         batch_size=batch_size, shuffle=True,\n",
        "                         collate_fn=lambda x: preprocess(*zip(*x)))"
      ],
      "metadata": {
        "id": "Dy0cDxC8E_uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationRNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(ClassificationRNN, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "\n",
        "        if type(hidden_dim) == int:\n",
        "          hidden_dim = [hidden_dim]\n",
        "\n",
        "        self.rnn = []\n",
        "        for i, hidd_d in enumerate(hidden_dim):\n",
        "          prev_d = embedding_dim if i == 0 else hidden_dim[i-1]\n",
        "          rnn = nn.LSTM(prev_d, hidd_d, batch_first=True)\n",
        "          self.add_module(f'lstm_{i}', rnn)\n",
        "          self.rnn.append(rnn)\n",
        "\n",
        "        self.fc = nn.Linear(hidd_d, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = F.relu(self.embedding(x))\n",
        "        for rnn in self.rnn:\n",
        "          embedded, (hidden_hn, hidden_cn) = rnn(embedded)\n",
        "\n",
        "        output = embedded\n",
        "        rnn_out = hidden_hn[0]  # hn\n",
        "        return self.fc(rnn_out)"
      ],
      "metadata": {
        "id": "jZo4-qLuEvKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "input_dim = tra_data[0].shape[1]\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256  # you can specify a list of number of units in sequential LSTM layers\n",
        "output_dim = 3\n",
        "\n",
        "model = ClassificationRNN(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Define the sparse cross-entropy loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 180\n",
        "\n",
        "tra_loss_hist = []\n",
        "val_loss_hist = []\n",
        "val_acc_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.\n",
        "    valid_loss = 0.\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        data, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    tra_loss_hist.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = []\n",
        "        for batch in test_loader:\n",
        "            data, labels = batch\n",
        "            output = model(data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            pred_class = torch.argmax(output, dim=1)\n",
        "            corr = pred_class == labels\n",
        "            correct.append(corr.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss /= len(test_loader)\n",
        "        correct = np.concatenate(correct)\n",
        "        accuracy = np.mean(correct)\n",
        "        print(f\"{epoch}:\\t Test loss: {valid_loss}; accuracy: {accuracy}\")\n",
        "\n",
        "        val_loss_hist.append(valid_loss)\n",
        "        val_acc_hist.append(accuracy)"
      ],
      "metadata": {
        "id": "KUgZVU4LIS2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss and accuracy on 2 subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(tra_loss_hist, label='train')\n",
        "ax1.plot(val_loss_hist, label='test')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(val_acc_hist)\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print(f'best validation accuracy: {np.max(val_acc_hist)} @ epoch {np.argmax(val_acc_hist)}')"
      ],
      "metadata": {
        "id": "GiXIspKVHcY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Exercise 1."
      ],
      "metadata": {
        "id": "ay-9UVS3I8AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work in 3 groups, use the 3 datasets.\n",
        "Optimize the model architecture: # layers, units per layer (hidden_dim) and the embedding_dim to improve validation accuracy."
      ],
      "metadata": {
        "id": "QWVq4Yp1I_v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Exercise 2."
      ],
      "metadata": {
        "id": "zQ7YqgUjJpgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your pretrained model from previous modules for embedding generation. Hint - you can use same Flair interface as for Glove and Roberta, just loading your model. E.g., given your model file 'my_model.pt', you can create document embedder:\n",
        "\n",
        "```\n",
        "my_model = torch.load('my_model.pt')\n",
        "document_embeddings_my_model = DocumentPoolEmbeddings([my_model])\n",
        "```"
      ],
      "metadata": {
        "id": "oIHJx96bJpgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternatively - you can combine the Glove embeddings and your emebedding, or the roberta ones:\n",
        "```\n",
        "document_embeddings_stacked = DocumentPoolEmbeddings([my_model, glove_embedding])\n",
        "```"
      ],
      "metadata": {
        "id": "jfKeQJujMSO2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7xmKKXAoQI3"
      },
      "source": [
        "# Thu evening (Transfer learning for PoS tagging/NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libs and utils"
      ],
      "metadata": {
        "id": "CYu4_7i4Jf-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ53du1av8Fz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBhg36Fnv2kf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import datasets\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from transformers import AutoModelForTokenClassification, AutoModel\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import evaluate\n",
        "\n",
        "import torch.cuda\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zxEZiBeEERL"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. load dataset"
      ],
      "metadata": {
        "id": "fm9QHx-NJmmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset info: https://huggingface.co/datasets/conll2003"
      ],
      "metadata": {
        "id": "v0bUkYgBJ0Um"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVNobEcEGZ4"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"conll2003\", num_proc=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLwa91VmEYIi"
      },
      "outputs": [],
      "source": [
        "len(dataset[\"train\"]), len(dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.keys()"
      ],
      "metadata": {
        "id": "vl3vcr_0LlDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tra = dataset['train']"
      ],
      "metadata": {
        "id": "omCh1nnrLnWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tra"
      ],
      "metadata": {
        "id": "7bLNKTtkL4q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tra.column_names"
      ],
      "metadata": {
        "id": "kiZ8_9oiLw6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tra[0]"
      ],
      "metadata": {
        "id": "Eu1Ss6JZMGAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html"
      ],
      "metadata": {
        "id": "fiuq90UihzC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u1gyjwpLr1F"
      },
      "outputs": [],
      "source": [
        "pos_tags_info = dataset['train'].features['pos_tags'].feature\n",
        "class_names = pos_tags_info.names\n",
        "\n",
        "class_idx_to_class_name = dict(enumerate(class_names))\n",
        "class_name_to_class_idx = {v:k for k, v in class_idx_to_class_name.items()}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "class_types = list(set(class_names))  # get unique\n",
        "\n",
        "class_idx_to_class_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load tokenizer"
      ],
      "metadata": {
        "id": "dyiJ5yoqKBbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the transformer library for most models you have the associated tokenizer.\n",
        "\n",
        "The methods AutoTokenizer and AutoModel instantiate a model of proper type."
      ],
      "metadata": {
        "id": "hA6M92lHKKOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3V-fo2rEzE_"
      },
      "outputs": [],
      "source": [
        "mod_name = 'bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1VHg7j0EuuG"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(mod_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "smgi6zkuKljj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "id": "n3lEZAZnKprO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(token_ids)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "br_d55KVfCSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Load model"
      ],
      "metadata": {
        "id": "1FE1arrrK2hc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgRxk1DyEqWB"
      },
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(mod_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1OkVnb0FVmJ"
      },
      "outputs": [],
      "source": [
        "example = dataset[\"test\"][560]\n",
        "example_txt = example['tokens']\n",
        "print(example_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlyC6OnMFtv6"
      },
      "outputs": [],
      "source": [
        "example_tokens_ids = tokenizer(example_txt, is_split_into_words=True, return_tensors='pt')\n",
        "example_tokens_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb1tAI3EHHba"
      },
      "outputs": [],
      "source": [
        "np_ids = example_tokens_ids['input_ids'].numpy()[0]\n",
        "tokens_r = tokenizer.convert_ids_to_tokens(np_ids)\n",
        "text_r = tokenizer.convert_tokens_to_string(tokens_r)\n",
        "text_r\n",
        "#tokens_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATNKII15F77m"
      },
      "outputs": [],
      "source": [
        "res = model(**example_tokens_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5G8II8vGPNI"
      },
      "outputs": [],
      "source": [
        "for k, v in res.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So.. What kind of model is this 'bert-base-uncased'?"
      ],
      "metadata": {
        "id": "F7UzACcKNwM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets use it for transfer learning - we use it's features to do token classification. For this we create a model of type `AutoModelForTokenClassification`:"
      ],
      "metadata": {
        "id": "aWmQ4jXjPw0H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os3DeFqgIDAb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(mod_name, num_labels=n_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxkHinpiIFbX"
      },
      "outputs": [],
      "source": [
        "res = model(**example_tokens_ids.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TOI-9lyIHQG"
      },
      "outputs": [],
      "source": [
        "for k, v in res.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Prepare dataset"
      ],
      "metadata": {
        "id": "j__GxfhGQabN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can use the tokenizer to covert text into token-ids. but there will be more of them than words. Our lables - are word based. We need thus to generate proper label - per token:"
      ],
      "metadata": {
        "id": "FPUkNon5RQZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LurjzhZPeAZ"
      },
      "outputs": [],
      "source": [
        "def tokenize_ner(examples, print_info=False):\n",
        "  # Extract word tokens and NER labels from examples\n",
        "  batch_words = examples['tokens']  # these are word tokens\n",
        "  batch_ner_tags = examples['pos_tags']  # examples['ner_tags']  # these are NER-labels per word\n",
        "\n",
        "  batch_tokens = tokenizer(batch_words, is_split_into_words=True, truncation=True)\n",
        "  batch_labels = []\n",
        "\n",
        "  LBL_IGN = -100  # label for tokens to be ignored during training\n",
        "\n",
        "  # Iterate through each example in the batch\n",
        "  for sample_idx, sample_ner_tags in enumerate(batch_ner_tags):\n",
        "    sample_labels = []  # To store NER labels for tokens in the current example\n",
        "\n",
        "    word_idxs = batch_tokens.word_ids(batch_index=sample_idx)\n",
        "    if print_info:\n",
        "      sample_token_ids = batch_tokens['input_ids'][sample_idx]\n",
        "      print(word_idxs, sample_ner_tags, tokenizer.convert_ids_to_tokens(sample_token_ids))\n",
        "\n",
        "    for word_idx in word_idxs:\n",
        "      if word_idx is None:  # Ignore tokens that are not related to real words\n",
        "        sample_labels.append(LBL_IGN)\n",
        "      else:\n",
        "        token_label = sample_ner_tags[word_idx]\n",
        "        sample_labels.append(token_label)  # Store the NER label for the token\n",
        "\n",
        "    # Add the NER labels for the current example to the batch_labels list\n",
        "    batch_labels.append(sample_labels)\n",
        "\n",
        "  # Add the batch_labels to the tokenized batch\n",
        "  batch_tokens['labels'] = batch_labels\n",
        "\n",
        "  return batch_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets inspect what and how does it do:"
      ],
      "metadata": {
        "id": "AAVoGp8mS0RE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQRFc-1ZP9uK"
      },
      "outputs": [],
      "source": [
        "res = tokenize_ner(dataset['train'][560:564], print_info=True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOMzChvMXU0c"
      },
      "outputs": [],
      "source": [
        "for token, token_id, label in zip(tokenizer.convert_ids_to_tokens(res['input_ids'][0]), res['input_ids'][0], res['labels'][0]):\n",
        "  print(f'{token:<15} {token_id:<10} {label}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9NLOrvpY1pG"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = dataset.map(tokenize_ner, batched=True, num_proc=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdRKBBXcZH8L"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets['train'][560]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whKgeLteb5aN"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets['train'][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaqitZaPb5h3"
      },
      "outputs": [],
      "source": [
        "res = model(input_ids=torch.tensor(tokenized_datasets['train'][560:561]['input_ids']).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGulWhZmb5h4"
      },
      "outputs": [],
      "source": [
        "for k, v in res.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly we need a collator.\n",
        "\n",
        "It ensures that tokenized inputs, labels, and other relevant data are properly formatted and batched together during training. This is particularly useful for tasks like Named Entity Recognition (NER) and part-of-speech tagging where token-level labels need to be aligned with input tokens."
      ],
      "metadata": {
        "id": "D2Ghd82sUDwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzf9Cq8rZ_j3"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD6h9ObGZ01o"
      },
      "source": [
        "### 5. Performnce evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80W52h6danh4"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"poseval\")  # seqeval for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm2gaNMIaniZ"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute evaluation metrics for a given batch of predictions and labels.\n",
        "# The function takes eval_pred as input, which contains batch_logits and batch_labels.\n",
        "def compute_metrics(eval_pred, print_info=False):\n",
        "    # Extract batch_logits and batch_labels from eval_pred.\n",
        "    batch_logits, batch_labels = eval_pred\n",
        "\n",
        "    # Compute batch_predictions by selecting the class with the highest probability.\n",
        "    batch_predictions = np.argmax(batch_logits, axis=-1)\n",
        "\n",
        "    # Initialize lists to store filtered predictions and labels for each sample in the batch.\n",
        "    filtered_hr_batch_predictions = []\n",
        "    filtered_hr_batch_labels = []\n",
        "\n",
        "    # Iterate over samples in the batch to filter out padding tokens (-100).\n",
        "    for sample_prediction, sample_label in zip(batch_predictions, batch_labels):\n",
        "        filtered_hr_sample_predictions = []\n",
        "        filtered_hr_sample_labels = []\n",
        "\n",
        "        # Iterate over predictions and labels in each sample.\n",
        "        for prediction, label in zip(sample_prediction, sample_label):\n",
        "            # Check if the label is not a padding token (-100).\n",
        "            if label != -100:\n",
        "                # Convert prediction and label indices to class names using class_idx_to_class_name mapping.\n",
        "                filtered_hr_sample_predictions.append(class_idx_to_class_name[prediction])\n",
        "                filtered_hr_sample_labels.append(class_idx_to_class_name[label])\n",
        "\n",
        "        # Append filtered predictions and labels for the sample to the batch lists.\n",
        "        filtered_hr_batch_predictions.append(filtered_hr_sample_predictions)\n",
        "        filtered_hr_batch_labels.append(filtered_hr_sample_labels)\n",
        "\n",
        "    # Compute evaluation metrics using the filtered predictions and labels.\n",
        "    metric_res = metric.compute(predictions=filtered_hr_batch_predictions,\n",
        "                                references=filtered_hr_batch_labels)\n",
        "\n",
        "    # Optionally print information about filtered predictions, labels, and metric results.\n",
        "    if print_info:\n",
        "        print(filtered_hr_batch_predictions)\n",
        "        print(filtered_hr_batch_labels)\n",
        "        print(metric_res)\n",
        "\n",
        "    # Create a dictionary to store computed metrics.\n",
        "    all_metrics = {k: v for k, v in metric_res.items() if type(v) is not dict}\n",
        "    all_metrics = {**all_metrics, **metric_res['weighted avg']}\n",
        "\n",
        "    # Compute and add F1 scores for specific class types to the metrics dictionary.\n",
        "    for ct in class_types:\n",
        "        v = metric_res.get(ct, None)\n",
        "        all_metrics[ct + '_f1'] = v['f1-score'] if v is not None else 0.\n",
        "\n",
        "    # Return the computed evaluation metrics.\n",
        "    return all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npn5MKzrlZ0e"
      },
      "outputs": [],
      "source": [
        "ds_test = tokenized_datasets['test']\n",
        "\n",
        "res = model(input_ids=torch.tensor(ds_test['input_ids'][:1]).to(device),\n",
        "            attention_mask=torch.tensor(ds_test['attention_mask'][:1]).to(device)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0Iqc1y6mMjn"
      },
      "outputs": [],
      "source": [
        "# don't forget to copy data to cpu-accessibel memory and convert to NumPy\n",
        "pred = res.logits.detach().cpu().numpy()\n",
        "lbl = tokenized_datasets['test']['labels'][:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUi4XrjhmZrk"
      },
      "outputs": [],
      "source": [
        "metrics_res = compute_metrics((pred, lbl), print_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqQUWG81piaU"
      },
      "outputs": [],
      "source": [
        "metrics_res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the last model layer doing the 47-way classification is not yet trained - the model spits out rubbish, and the scores are 0"
      ],
      "metadata": {
        "id": "BLF6WmeeWTxW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4NUQNQuWfAC"
      },
      "source": [
        "### 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vo2DcBkania"
      },
      "outputs": [],
      "source": [
        "# rm -rf ner logs_ner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf pos logs_pos"
      ],
      "metadata": {
        "id": "TOyWBrYOr8JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgrt5Y8Oania"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=\"pos\",\n",
        "                                  num_train_epochs=10,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "\n",
        "                                  per_device_train_batch_size=16,  # batch size per device during training\n",
        "                                  per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "                                  warmup_steps=250,                # number of warmup steps for learning rate scheduler\n",
        "                                  weight_decay=0.01,               # strength of weight decay\n",
        "                                  logging_dir='./logs_pos',        # directory for storing logs\n",
        "                                  logging_steps=10,\n",
        "                                  # optim=\"adafactor\"\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glhIGAlvania"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "xC9QaanQc1DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idpSRht2anic"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HTQQrTbxFX4"
      },
      "outputs": [],
      "source": [
        "def plot_hist(log_hist):\n",
        "  \"\"\"\n",
        "  Helper function to aggregate and visualize training history\n",
        "  from trainers's logs\n",
        "  \"\"\"\n",
        "  last_loss = 0\n",
        "  s = 4\n",
        "  sfx = 'eval_'\n",
        "  e_loss_name = sfx+'loss'\n",
        "  sfx_skip = ['_per_second', 'runtime', 'epoch', 'step', 'confusion_matrix']\n",
        "\n",
        "  steps = []\n",
        "  loss = []\n",
        "\n",
        "  e_steps = []\n",
        "  e_loss = []\n",
        "  e_mtr = {}\n",
        "\n",
        "  e_cms = []\n",
        "\n",
        "  for el in log_hist:\n",
        "    if e_loss_name in el:\n",
        "      for k, v in el.items():\n",
        "        if any([s in k for s in sfx_skip]):\n",
        "          continue\n",
        "\n",
        "\n",
        "        if k == e_loss_name:\n",
        "          e_loss.append(v)\n",
        "        else:\n",
        "          if k not in e_mtr:\n",
        "            e_mtr[k] = []\n",
        "          e_mtr[k].append(v)\n",
        "\n",
        "      e_steps.append(el['step'])\n",
        "      if 'confusion_matrix' in el:\n",
        "        e_cms.append(el['confusion_matrix'])\n",
        "\n",
        "    else:\n",
        "      if 'loss' in el:\n",
        "        steps.append(el['step'])\n",
        "        loss.append(el['loss'])\n",
        "\n",
        "  n_fig = len(e_mtr)+1\n",
        "\n",
        "  fix, ax = plt.subplots(1, n_fig, figsize=(s*n_fig, s*1))\n",
        "  ax[0].plot(steps, loss, alpha=0.5, label='tra')\n",
        "  ax[0].plot(e_steps, e_loss, alpha=0.5, label='val')\n",
        "  ax[0].set_xlabel('steps')\n",
        "  ax[0].set_title('loss')\n",
        "  ax[0].legend()\n",
        "\n",
        "  for idx, (lbl, vals) in enumerate(e_mtr.items()):\n",
        "    i = idx+1\n",
        "\n",
        "    #print(lbl, vals, e_steps)\n",
        "    ax[i].plot(e_steps, vals, alpha=0.5, label='val')\n",
        "    ax[i].set_xlabel('steps')\n",
        "    ax[i].set_title(lbl)\n",
        "\n",
        "    #ax[i].legend()\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  for i, cm in enumerate(e_cms):\n",
        "    plt.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
        "    plt.title(e_steps[i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2ut-GVBk7ID"
      },
      "outputs": [],
      "source": [
        "plot_hist(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Exercise"
      ],
      "metadata": {
        "id": "SUUPCgbXXF9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try modifying the code to do NER tag identification"
      ],
      "metadata": {
        "id": "D4FJNA65eo_8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4SFDHxitoA-5",
        "vkk3qvativ-g",
        "A_4JM1eMRZBB",
        "F-GBDk-EgBUL",
        "kRTNzxvNRcjl",
        "rqwx5y_dRii4",
        "RoW-n86KRijC",
        "_D-dicgbRsin",
        "x0W2XZkIdyjJ",
        "PzNn1ybJf2TO",
        "p1nUMIbUoLl-",
        "lwxYyCso5IFa",
        "2wajOn8m5Oq8",
        "rP4-PTgK5jbN",
        "I4Zi-tyR5tNX",
        "G-NUgYrD6Dfg",
        "AtwfrYsDoKVX",
        "wj0VGjy38y_d",
        "XX6Kx_zwBb8e",
        "EV15Ei_70NH0",
        "cAafXxdv_XNE",
        "oC5fQSXhJwMv",
        "9JMEoreu_hly",
        "sHqmaxTw7Myf",
        "apoYWHguBV9C",
        "bi1QNGL5BNZS",
        "ccbxW2q4BgA0",
        "CsTsJdNMBlhL",
        "CirttWtLCH3T",
        "ay-9UVS3I8AL",
        "zQ7YqgUjJpgW",
        "E7xmKKXAoQI3",
        "CYu4_7i4Jf-c",
        "fm9QHx-NJmmX",
        "dyiJ5yoqKBbp",
        "1FE1arrrK2hc",
        "j__GxfhGQabN",
        "JD6h9ObGZ01o",
        "H4NUQNQuWfAC",
        "SUUPCgbXXF9F"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}